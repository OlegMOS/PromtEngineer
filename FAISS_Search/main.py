import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import re
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import nest_asyncio
import asyncio
import os
from dotenv import load_dotenv
import pickle
import hashlib
import time

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è nest_asyncio –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ Jupyter/Colab
nest_asyncio.apply()


class ResumeSearchEngine:
    def __init__(self, excel_file_path):
        self.df = pd.read_excel(excel_file_path)
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.index = None
        self.resume_texts = []
        self.cache_dir = "vector_cache"
        self.excel_file_path = excel_file_path

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

    def _get_cache_key(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –º–æ–¥–µ–ª–∏"""
        # –•—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
        file_hash = hashlib.md5()
        with open(self.excel_file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                file_hash.update(chunk)

        # –•—ç—à –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        model_hash = hashlib.md5(self.model.get_sentence_embedding_dimension().__str__().encode()).hexdigest()[:8]

        cache_key = f"{file_hash.hexdigest()}_{model_hash}"
        return cache_key

    def _cache_exists(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞"""
        cache_key = self._get_cache_key()
        index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
        texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
        metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")

        return all(os.path.exists(path) for path in [index_path, texts_path, metadata_path])

    def _save_to_cache(self, embeddings):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à"""
        cache_key = self._get_cache_key()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
        faiss.write_index(self.index, index_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Ä–µ–∑—é–º–µ
        texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
        with open(texts_path, 'wb') as f:
            pickle.dump(self.resume_texts, f)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")
        metadata = {
            'timestamp': time.time(),
            'data_shape': embeddings.shape,
            'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'num_resumes': len(self.resume_texts)
        }
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

        print(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à: {cache_key}")

    def _load_from_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞"""
        cache_key = self._get_cache_key()

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
            self.index = faiss.read_index(index_path)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Ä–µ–∑—é–º–µ
            texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
            with open(texts_path, 'rb') as f:
                self.resume_texts = pickle.load(f)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)

            print(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞: {cache_key}")
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.resume_texts)} —Ä–µ–∑—é–º–µ, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.index.d}")
            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫—ç—à–∞: {e}")
            return False

    def preprocess_data(self):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        text_columns = ['positionName', 'experience', 'educationList', 'workExperienceList',
                        'hardSkills', 'softSkills', 'scheduleType', 'busyType']

        for col in text_columns:
            if col in self.df.columns:
                self.df[col] = self.df[col].fillna('')

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—é–º–µ
        for idx, row in self.df.iterrows():
            resume_text = f"–î–æ–ª–∂–Ω–æ—Å—Ç—å: {row.get('positionName', '')}. "

            if row.get('experience', ''):
                resume_text += f"–û–ø—ã—Ç: {row['experience']} –ª–µ—Ç. "

            # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            if pd.notna(row.get('educationList')) and row['educationList']:
                edu_text = str(row['educationList']).replace('[', '').replace(']', '').replace('{', '').replace('}', '')
                resume_text += f"–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: {edu_text}. "

            # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
            if pd.notna(row.get('workExperienceList')) and row['workExperienceList']:
                exp_text = str(row['workExperienceList'])[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                resume_text += f"–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã: {exp_text}. "

            # –ù–∞–≤—ã–∫–∏
            if pd.notna(row.get('hardSkills')) and row['hardSkills']:
                skills_text = str(row['hardSkills']).replace('[', '').replace(']', '').replace('{', '').replace('}', '')
                resume_text += f"–ù–∞–≤—ã–∫–∏: {skills_text}. "

            # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            if pd.notna(row.get('localityName')) and row['localityName']:
                location = str(row['localityName']).replace('-', ' ')
                resume_text += f"–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: {location}. "

            self.resume_texts.append(resume_text)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.resume_texts)} —Ä–µ–∑—é–º–µ")

    def create_faiss_index(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö FAISS"""

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
        if self._cache_exists():
            if self._load_from_cache():
                return

        print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π...")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = self.model.encode(self.resume_texts, show_progress_bar=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(embeddings)

        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # IndexFlatIP –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        self.index.add(embeddings.astype('float32'))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self._save_to_cache(embeddings)

        print(f"–ò–Ω–¥–µ–∫—Å FAISS —Å–æ–∑–¥–∞–Ω —Å {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–∞–º–∏")

    def clear_old_cache(self, max_age_days=30):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∫—ç—à-—Ñ–∞–π–ª–æ–≤"""
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 60 * 60

        for filename in os.listdir(self.cache_dir):
            filepath = os.path.join(self.cache_dir, filename)
            if os.path.isfile(filepath):
                file_age = current_time - os.path.getmtime(filepath)
                if file_age > max_age_seconds:
                    os.remove(filepath)
                    print(f"–£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: {filename}")

    def keyword_search(self, query, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # –ü–æ–∏—Å–∫ –≤ FAISS
        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.df):
                results.append({
                    'index': idx,
                    'score': float(score),
                    'type': 'keyword'
                })

        return results

    def location_search(self, location_query, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏"""
        location_results = []

        for idx, row in self.df.iterrows():
            if pd.notna(row.get('localityName')):
                location = str(row['localityName']).lower().replace('-', ' ')
                query = location_query.lower()

                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é
                if query in location:
                    score = 1.0
                else:
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤
                    location_words = set(location.split())
                    query_words = set(query.split())
                    common_words = location_words.intersection(query_words)

                    if common_words:
                        score = len(common_words) / len(query_words)
                    else:
                        score = 0.0

                if score > 0:
                    location_results.append({
                        'index': idx,
                        'score': score,
                        'type': 'location'
                    })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        location_results.sort(key=lambda x: x['score'], reverse=True)
        return location_results[:k]

    def hybrid_search(self, keyword_query, location_query, k=5, keyword_weight=0.3, location_weight=0.7):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –ª–æ–∫–∞—Ü–∏–∏, –∑–∞—Ç–µ–º –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏"""
        print(f"–ü–æ–∏—Å–∫: '{keyword_query}' –≤ –ª–æ–∫–∞—Ü–∏–∏ '{location_query}'")

        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        location_results = self.location_search(location_query, k=50)  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

        if not location_results:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ª–æ–∫–∞—Ü–∏–∏, –∏—â–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏")
            keyword_results = self.keyword_search(keyword_query, k=k)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç
            final_results = []
            for result in keyword_results:
                final_results.append({
                    'index': result['index'],
                    'keyword_score': result['score'],
                    'location_score': 0,
                    'final_score': result['score'] * keyword_weight
                })

            return final_results[:k]

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ª–æ–∫–∞—Ü–∏–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–º score –ª–æ–∫–∞—Ü–∏–∏)
        min_location_score = 0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        filtered_location_results = [r for r in location_results if r['score'] >= min_location_score]

        if not filtered_location_results:
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å–ª–∞–±–ª—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–π
            filtered_location_results = location_results[:10]  # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –ø–æ –ª–æ–∫–∞—Ü–∏–∏

        # –°—Ä–µ–¥–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –ª–æ–∫–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ –∏—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        location_indices = [r['index'] for r in filtered_location_results]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ —ç—Ç–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        keyword_results = self._keyword_search_among_indices(keyword_query, location_indices, k=len(location_indices))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_results = {}

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ª–æ–∫–∞—Ü–∏–∏
        location_dict = {r['index']: r for r in filtered_location_results}

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º score
        for keyword_result in keyword_results:
            idx = keyword_result['index']
            if idx in location_dict:
                location_score = location_dict[idx]['score']
                keyword_score = keyword_result['score']

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º scores
                normalized_keyword = keyword_score
                normalized_location = location_score

                combined_results[idx] = {
                    'index': idx,
                    'keyword_score': normalized_keyword,
                    'location_score': normalized_location,
                    'final_score': (normalized_keyword * keyword_weight + normalized_location * location_weight)
                }

        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ —Ç–æ–ª—å–∫–æ –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        if len(combined_results) < k:
            for location_result in filtered_location_results:
                idx = location_result['index']
                if idx not in combined_results:
                    combined_results[idx] = {
                        'index': idx,
                        'keyword_score': 0,
                        'location_score': location_result['score'],
                        'final_score': location_result['score'] * location_weight
                    }

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É score
        final_results = sorted(combined_results.values(),
                               key=lambda x: (x['final_score'], x['location_score'], x['keyword_score']),
                               reverse=True)

        return final_results[:k]

    def _keyword_search_among_indices(self, query, indices, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        if not indices:
            return []

        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        all_embeddings = []
        valid_indices = []

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ
        dimension = self.index.d
        temp_index = faiss.IndexFlatIP(dimension)

        for idx in indices:
            if idx < self.index.ntotal:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                embedding = self.index.reconstruct(idx).reshape(1, -1)
                temp_index.add(embedding.astype('float32'))
                valid_indices.append(idx)

        if temp_index.ntotal == 0:
            return []

        # –ò—â–µ–º –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏–Ω–¥–µ–∫—Å–µ
        scores, temp_indices = temp_index.search(query_embedding.astype('float32'), min(k, temp_index.ntotal))

        results = []
        for score, temp_idx in zip(scores[0], temp_indices[0]):
            if temp_idx < len(valid_indices):
                original_idx = valid_indices[temp_idx]
                results.append({
                    'index': original_idx,
                    'score': float(score),
                    'type': 'keyword'
                })

        return results

    def get_resume_details(self, index):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π —Ä–µ–∑—é–º–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if index >= len(self.df):
            return None

        row = self.df.iloc[index]

        details = {
            'id': row.get('id', ''),
            'position': row.get('positionName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'location': row.get('localityName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'age': row.get('age', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'experience': row.get('experience', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'education': self.extract_education(row.get('educationList', '')),
            'skills': self.extract_skills(row.get('hardSkills', '')),
            'schedule': row.get('scheduleType', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'salary': row.get('salary', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'relocation': row.get('relocation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
        }

        return details

    def extract_education(self, education_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏"""
        if not education_data or education_data == '[]':
            return "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

        try:
            if isinstance(education_data, str) and 'instituteName' in education_data:
                # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞
                match = re.search(r"'instituteName': '([^']*)'", str(education_data))
                if match:
                    return match.group(1)
            return str(education_data)[:100] + "..." if len(str(education_data)) > 100 else str(education_data)
        except:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏"

    def extract_skills(self, skills_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–∞–≤—ã–∫–∞—Ö"""
        if not skills_data or skills_data == '[]':
            return "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

        try:
            if isinstance(skills_data, str) and 'hardSkillName' in skills_data:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞–≤—ã–∫–æ–≤
                skills = re.findall(r"'hardSkillName': '([^']*)'", str(skills_data))
                if skills:
                    return ", ".join(skills[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–≤—ã–∫–æ–≤
            return str(skills_data)[:100] + "..." if len(str(skills_data)) > 100 else str(skills_data)
        except:
            return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏"


# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
TELEGRAM_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')

if not TELEGRAM_TOKEN:
    raise ValueError("TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã...")
search_engine = ResumeSearchEngine('–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.xlsx')
search_engine.preprocess_data()
search_engine.create_faiss_index()

# –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –∫—ç—à–∞ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
search_engine.clear_old_cache(max_age_days=30)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    welcome_text = """
ü§ñ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –±–æ—Ç –ø–æ–∏—Å–∫–∞ —Ä–µ–∑—é–º–µ!

–î–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –≤–æ–¥–∏—Ç–µ–ª—å; –ú–æ—Å–∫–≤–∞; 5
    """
    await update.message.reply_text(welcome_text)


async def handle_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        user_input = update.message.text
        parts = user_input.split(';')

        if len(parts) < 3:
            await update.message.reply_text("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: [–¥–æ–ª–∂–Ω–æ—Å—Ç—å]; [–≥–æ—Ä–æ–¥]; [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ]")
            return

        keyword = parts[0].strip()
        location = parts[1].strip()

        try:
            k = int(parts[2].strip())
            k = min(k, 10)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        except:
            k = 5

        if not keyword or not location:
            await update.message.reply_text("‚ùå –£–∫–∞–∂–∏—Ç–µ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ")
            return

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        await update.message.reply_text("üîç –ò—â—É –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ä–µ–∑—é–º–µ...")

        results = search_engine.hybrid_search(keyword, location, k=k)

        if not results:
            await update.message.reply_text("‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(results)}\n\n"

        for i, result in enumerate(results, 1):
            details = search_engine.get_resume_details(result['index'])

            if details:
                response += f"üèÜ **–†–µ–∑—é–º–µ #{i}** (score: {result['final_score']:.2f})\n"
                response += f"üíº –î–æ–ª–∂–Ω–æ—Å—Ç—å: {details['position']}\n"
                response += f"üìç –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: {details['location']}\n"
                response += f"üë§ –í–æ–∑—Ä–∞—Å—Ç: {details['age']}\n"
                response += f"üìÖ –û–ø—ã—Ç: {details['experience']} –ª–µ—Ç\n"
                response += f"üéì –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: {details['education']}\n"
                response += f"üí∞ –ó–∞—Ä–ø–ª–∞—Ç–∞: {details['salary']}\n"
                response += f"üïí –ì—Ä–∞—Ñ–∏–∫: {details['schedule']}\n"
                response += f"üöó –ü–µ—Ä–µ–µ–∑–¥: {details['relocation']}\n"

                if i < len(results):
                    response += "‚îÄ" * 30 + "\n\n"

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
        if len(response) > 4096:
            parts = [response[i:i + 4096] for i in range(0, len(response), 4096)]
            for part in parts:
                await update.message.reply_text(part)
        else:
            await update.message.reply_text(response)

    except Exception as e:
        await update.message.reply_text(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫"""
    print(f"–û—à–∏–±–∫–∞: {context.error}")
    await update.message.reply_text("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_search))
    application.add_error_handler(error_handler)

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    application.run_polling()


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–∞
if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    print("–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã:")

    # –ü—Ä–∏–º–µ—Ä 1: –ü–æ–∏—Å–∫ –≤–æ–¥–∏—Ç–µ–ª–µ–π –≤ –ú–æ—Å–∫–≤–µ
    results = search_engine.hybrid_search("–≤–æ–¥–∏—Ç–µ–ª—å", "–ú–æ—Å–∫–≤–∞", k=3)
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—é–º–µ –≤–æ–¥–∏—Ç–µ–ª–µ–π –≤ –ú–æ—Å–∫–≤–µ:")
    for i, result in enumerate(results, 1):
        details = search_engine.get_resume_details(result['index'])
        print(f"{i}. {details['position']} - {details['location']} (score: {result['final_score']:.2f})")

    # –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ
    results = search_engine.hybrid_search("–ø—Ä–æ–¥–∞–≤–µ—Ü", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", k=2)
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—é–º–µ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ:")
    for i, result in enumerate(results, 1):
        details = search_engine.get_resume_details(result['index'])
        print(f"{i}. {details['position']} - {details['location']} (score: {result['final_score']:.2f})")

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    main()