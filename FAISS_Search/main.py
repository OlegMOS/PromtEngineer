import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import re
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import nest_asyncio
import asyncio
import os
from dotenv import load_dotenv
import pickle
import hashlib
import time
from collections import Counter
from rapidfuzz import fuzz, process

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è nest_asyncio –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ Jupyter/Colab
nest_asyncio.apply()


class SimpleRussianStemmer:
    """–ü—Ä–æ—Å—Ç–æ–π —Å—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""

    def __init__(self):
        # –û–∫–æ–Ω—á–∞–Ω–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
        self.endings = [
            '–æ–≤—Å–∫–∏–π', '–µ–≤—Å–∫–∏–π', '–∏–Ω—Å–∫–∏–π', '–µ–Ω—Å–∫–∏–π', '—ã–π', '–æ–π', '–∞—è', '—è—è', '–æ–µ', '–µ–µ',  # –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
            '–æ—Å—Ç—å', '–µ–π', '–∞', '—è', '–æ', '–µ', '—å', '–∏', '—ã', '—É', '—é', '–µ–º', '–æ–º', '–∞–º–∏', '—è–º–∏'  # –ø–∞–¥–µ–∂–∏
        ]

    def stem(self, word):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Å–Ω–æ–≤—É —Å–ª–æ–≤–∞"""
        if len(word) < 3:
            return word

        word_lower = word.lower()

        # –£–¥–∞–ª—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è
        for ending in self.endings:
            if word_lower.endswith(ending) and len(word_lower) > len(ending) + 2:
                return word_lower[:-len(ending)]

        return word_lower


class ResumeSearchEngine:
    def __init__(self, excel_file_path):
        self.df = pd.read_excel(excel_file_path)
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.index = None
        self.resume_texts = []
        self.cache_dir = "vector_cache"
        self.excel_file_path = excel_file_path
        self.stemmer = SimpleRussianStemmer()

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        self.position_variations = self._extract_position_variations()

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

    def _extract_position_variations(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        position_variations = {}

        if 'positionName' not in self.df.columns:
            return position_variations

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
        all_positions = self.df['positionName'].dropna().unique()

        for position in all_positions:
            if isinstance(position, str) and position.strip():
                pos_lower = position.lower().strip()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤—É –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
                words = self._extract_words(pos_lower)
                if words:
                    main_stem = words[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Å—Ç–µ–º –∫–∞–∫ –æ—Å–Ω–æ–≤—É

                    if main_stem not in position_variations:
                        position_variations[main_stem] = set()

                    position_variations[main_stem].add(pos_lower)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–∫–∏
        return {k: list(v) for k, v in position_variations.items()}

    def _extract_words(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ —Å—Ç–µ–º–º–∏—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ —Ü–∏—Ñ—Ä—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã
        words = re.findall(r'[–∞-—è—ë]+', text.lower())
        # –°—Ç–µ–º–º–∏–º —Å–ª–æ–≤–∞ –∏ —É–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
        stemmed_words = [self.stemmer.stem(word) for word in words if len(word) >= 3]
        return stemmed_words

    def _get_cache_key(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –º–æ–¥–µ–ª–∏"""
        # –•—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
        file_hash = hashlib.md5()
        with open(self.excel_file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                file_hash.update(chunk)

        # –•—ç—à –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        model_hash = hashlib.md5(self.model.get_sentence_embedding_dimension().__str__().encode()).hexdigest()[:8]

        cache_key = f"{file_hash.hexdigest()}_{model_hash}"
        return cache_key

    def _cache_exists(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞"""
        cache_key = self._get_cache_key()
        index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
        texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
        metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")

        return all(os.path.exists(path) for path in [index_path, texts_path, metadata_path])

    def _save_to_cache(self, embeddings):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à"""
        cache_key = self._get_cache_key()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
        faiss.write_index(self.index, index_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Ä–µ–∑—é–º–µ
        texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
        with open(texts_path, 'wb') as f:
            pickle.dump(self.resume_texts, f)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")
        metadata = {
            'timestamp': time.time(),
            'data_shape': embeddings.shape,
            'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'num_resumes': len(self.resume_texts)
        }
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

        print(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à: {cache_key}")

    def _load_from_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞"""
        cache_key = self._get_cache_key()

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(self.cache_dir, f"{cache_key}.faiss")
            self.index = faiss.read_index(index_path)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Ä–µ–∑—é–º–µ
            texts_path = os.path.join(self.cache_dir, f"{cache_key}_texts.pkl")
            with open(texts_path, 'rb') as f:
                self.resume_texts = pickle.load(f)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            metadata_path = os.path.join(self.cache_dir, f"{cache_key}_metadata.pkl")
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)

            print(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞: {cache_key}")
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.resume_texts)} —Ä–µ–∑—é–º–µ, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.index.d}")
            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫—ç—à–∞: {e}")
            return False

    def preprocess_data(self):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        text_columns = ['positionName', 'experience', 'educationList', 'workExperienceList',
                        'hardSkills', 'softSkills', 'scheduleType', 'busyType']

        for col in text_columns:
            if col in self.df.columns:
                self.df[col] = self.df[col].fillna('')

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—é–º–µ
        for idx, row in self.df.iterrows():
            resume_text = f"–î–æ–ª–∂–Ω–æ—Å—Ç—å: {row.get('positionName', '')}. "

            if row.get('experience', ''):
                resume_text += f"–û–ø—ã—Ç: {row['experience']} –ª–µ—Ç. "

            # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            if pd.notna(row.get('educationList')) and row['educationList']:
                edu_text = str(row['educationList']).replace('[', '').replace(']', '').replace('{', '').replace('}', '')
                resume_text += f"–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: {edu_text}. "

            # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
            if pd.notna(row.get('workExperienceList')) and row['workExperienceList']:
                exp_text = str(row['workExperienceList'])[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                resume_text += f"–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã: {exp_text}. "

            # –ù–∞–≤—ã–∫–∏
            if pd.notna(row.get('hardSkills')) and row['hardSkills']:
                skills_text = str(row['hardSkills']).replace('[', '').replace(']', '').replace('{', '').replace('}', '')
                resume_text += f"–ù–∞–≤—ã–∫–∏: {skills_text}. "

            # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            if pd.notna(row.get('localityName')) and row['localityName']:
                location = str(row['localityName']).replace('-', ' ')
                resume_text += f"–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: {location}. "

            self.resume_texts.append(resume_text)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.resume_texts)} —Ä–µ–∑—é–º–µ")
        print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(self.position_variations)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π")

    def create_faiss_index(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö FAISS"""

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
        if self._cache_exists():
            if self._load_from_cache():
                return

        print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π...")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = self.model.encode(self.resume_texts, show_progress_bar=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(embeddings)

        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # IndexFlatIP –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        self.index.add(embeddings.astype('float32'))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self._save_to_cache(embeddings)

        print(f"–ò–Ω–¥–µ–∫—Å FAISS —Å–æ–∑–¥–∞–Ω —Å {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–∞–º–∏")

    def clear_old_cache(self, max_age_days=30):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∫—ç—à-—Ñ–∞–π–ª–æ–≤"""
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 60 * 60

        for filename in os.listdir(self.cache_dir):
            filepath = os.path.join(self.cache_dir, filename)
            if os.path.isfile(filepath):
                file_age = current_time - os.path.getmtime(filepath)
                if file_age > max_age_seconds:
                    os.remove(filepath)
                    print(f"–£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: {filename}")

    def fuzzy_position_search(self, position_query, k=10):
        """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fuzzy matching"""
        position_results = []

        if 'positionName' not in self.df.columns:
            return position_results

        query_lower = position_query.lower().strip()
        query_stems = self._extract_words(query_lower)

        if not query_stems:
            return position_results

        for idx, row in self.df.iterrows():
            position = str(row.get('positionName', '')).lower()
            if not position:
                continue

            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
            if query_lower in position or position in query_lower:
                score = 1.0
            else:
                # 2. Fuzzy matching
                fuzzy_score = fuzz.partial_ratio(query_lower, position) / 100.0

                # 3. –°—Ç–µ–º-—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                position_stems = self._extract_words(position)
                stem_overlap = len(set(query_stems) & set(position_stems)) / len(query_stems) if query_stems else 0

                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
                score = max(fuzzy_score, stem_overlap * 0.8)

            if score > 0.3:  # –ü–æ—Ä–æ–≥ –¥–ª—è —É—á–µ—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                position_results.append({
                    'index': idx,
                    'score': score,
                    'type': 'position'
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        position_results.sort(key=lambda x: x['score'], reverse=True)
        return position_results[:k]

    def keyword_search(self, query, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # –ü–æ–∏—Å–∫ –≤ FAISS
        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.df):
                results.append({
                    'index': idx,
                    'score': float(score),
                    'type': 'keyword'
                })

        return results

    def location_search(self, location_query, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
        location_results = []

        # –°—Ç–µ–º–º–∏–º —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_stems = self._extract_words(location_query)

        if not query_stems:
            return location_results

        for idx, row in self.df.iterrows():
            if pd.notna(row.get('localityName')):
                location = str(row['localityName'])
                location_stems = self._extract_words(location)

                if not location_stems:
                    continue

                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–µ–º–º–æ–≤
                query_counter = Counter(query_stems)
                location_counter = Counter(location_stems)

                # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ —Å—Ç–µ–º–º—ã
                common_stems = set(query_stems).intersection(set(location_stems))

                if common_stems:
                    # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    total_query_weight = sum(query_counter[stem] for stem in query_stems)
                    common_weight = sum(min(query_counter[stem], location_counter[stem]) for stem in common_stems)

                    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–∞–∫–∂–µ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    partial_matches = 0
                    for q_stem in query_stems:
                        for l_stem in location_stems:
                            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–æ–¥–∏–Ω —Å—Ç–µ–º–º —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º)
                            if q_stem in l_stem or l_stem in q_stem:
                                partial_matches += 0.3
                                break

                    score = (common_weight / total_query_weight) + (partial_matches / len(query_stems))
                    score = min(score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score

                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    exact_match = any(q_stem == l_stem for q_stem in query_stems for l_stem in location_stems)
                    if exact_match:
                        score = min(score + 0.2, 1.0)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    partial_matches = 0
                    for q_stem in query_stems:
                        for l_stem in location_stems:
                            if q_stem in l_stem or l_stem in q_stem:
                                partial_matches += 1
                                break

                    score = partial_matches / len(query_stems) if query_stems else 0

                if score > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    location_results.append({
                        'index': idx,
                        'score': score,
                        'type': 'location'
                    })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        location_results.sort(key=lambda x: x['score'], reverse=True)
        return location_results[:k]

    def location_priority_search(self, keyword_query, location_query, k=5):
        """–ü–æ–∏—Å–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ª–æ–∫–∞—Ü–∏–∏: —Å–Ω–∞—á–∞–ª–∞ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ª–æ–∫–∞—Ü–∏–∏, –∑–∞—Ç–µ–º –∏—â–µ–º –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏"""
        print(f"–ü–æ–∏—Å–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ª–æ–∫–∞—Ü–∏–∏: '{keyword_query}' –≤ '{location_query}'")

        # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –ø–æ –ª–æ–∫–∞—Ü–∏–∏ (–ø–µ—Ä–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        location_results = self.location_search(location_query, k=50)

        if not location_results:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏")
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ª–æ–∫–∞—Ü–∏–∏, –∏—â–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keyword_results = self.keyword_search(keyword_query, k=k)
            position_results = self.fuzzy_position_search(keyword_query, k=k)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
            all_profession_results = {}
            for result in keyword_results + position_results:
                idx = result['index']
                if idx not in all_profession_results or result['score'] > all_profession_results[idx]['score']:
                    all_profession_results[idx] = {
                        'index': idx,
                        'profession_score': result['score'],
                        'location_score': 0
                    }

            final_results = []
            for result in all_profession_results.values():
                final_score = result['profession_score'] * 0.7  # –í–µ—Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∫–æ–≥–¥–∞ –Ω–µ—Ç –ª–æ–∫–∞—Ü–∏–∏
                final_results.append({
                    'index': result['index'],
                    'profession_score': result['profession_score'],
                    'location_score': 0,
                    'final_score': final_score
                })

            final_results.sort(key=lambda x: x['final_score'], reverse=True)
            return final_results[:k]

        # –®–∞–≥ 2: –°—Ä–µ–¥–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ª–æ–∫–∞—Ü–∏–∏ –∏—â–µ–º –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
        location_indices = [r['index'] for r in location_results]

        # –ü–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –ª–æ–∫–∞—Ü–∏–∏
        profession_results = []

        # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
        position_results = self.fuzzy_position_search(keyword_query, k=len(location_indices))
        position_results = [r for r in position_results if r['index'] in location_indices]

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_results = self._keyword_search_among_indices(keyword_query, location_indices, k=len(location_indices))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
        profession_scores = {}
        for result in position_results + keyword_results:
            idx = result['index']
            if idx not in profession_scores or result['score'] > profession_scores[idx]:
                profession_scores[idx] = result['score']

        # –®–∞–≥ 3: –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ü–µ–Ω–∫–∏ –ª–æ–∫–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
        location_dict = {r['index']: r['score'] for r in location_results}

        final_results = []
        for idx in location_indices:
            location_score = location_dict.get(idx, 0)
            profession_score = profession_scores.get(idx, 0)

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ª–æ–∫–∞—Ü–∏–∏: location_weight = 0.7, profession_weight = 0.3
            final_score = (location_score * 0.7) + (profession_score * 0.3)

            final_results.append({
                'index': idx,
                'profession_score': profession_score,
                'location_score': location_score,
                'final_score': final_score
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é final_score
        final_results.sort(key=lambda x: x['final_score'], reverse=True)

        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–Ω—å—à–µ k, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ —Å –≤—ã—Å–æ–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π –ª–æ–∫–∞—Ü–∏–∏
        if len(final_results) < k:
            for location_result in location_results:
                idx = location_result['index']
                if idx not in [r['index'] for r in final_results]:
                    final_results.append({
                        'index': idx,
                        'profession_score': 0,
                        'location_score': location_result['score'],
                        'final_score': location_result['score'] * 0.7
                    })

        return final_results[:k]

    def _keyword_search_among_indices(self, query, indices, k=10):
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        if not indices:
            return []

        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        all_embeddings = []
        valid_indices = []

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ
        dimension = self.index.d
        temp_index = faiss.IndexFlatIP(dimension)

        for idx in indices:
            if idx < self.index.ntotal:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                embedding = self.index.reconstruct(idx).reshape(1, -1)
                temp_index.add(embedding.astype('float32'))
                valid_indices.append(idx)

        if temp_index.ntotal == 0:
            return []

        # –ò—â–µ–º –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏–Ω–¥–µ–∫—Å–µ
        scores, temp_indices = temp_index.search(query_embedding.astype('float32'), min(k, temp_index.ntotal))

        results = []
        for score, temp_idx in zip(scores[0], temp_indices[0]):
            if temp_idx < len(valid_indices):
                original_idx = valid_indices[temp_idx]
                results.append({
                    'index': original_idx,
                    'score': float(score),
                    'type': 'keyword'
                })

        return results

    def get_resume_details(self, index):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π —Ä–µ–∑—é–º–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if index >= len(self.df):
            return None

        row = self.df.iloc[index]

        details = {
            'id': row.get('id', ''),
            'position': row.get('positionName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'location': row.get('localityName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'age': row.get('age', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'experience': row.get('experience', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'education': self.extract_education(row.get('educationList', '')),
            'skills': self.extract_skills(row.get('hardSkills', '')),
            'schedule': row.get('scheduleType', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'salary': row.get('salary', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'relocation': row.get('relocation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
        }

        return details

    def extract_education(self, education_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏"""
        if not education_data or education_data == '[]':
            return "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

        try:
            if isinstance(education_data, str) and 'instituteName' in education_data:
                # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞
                match = re.search(r"'instituteName': '([^']*)'", str(education_data))
                if match:
                    return match.group(1)
            return str(education_data)[:100] + "..." if len(str(education_data)) > 100 else str(education_data)
        except:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏"

    def extract_skills(self, skills_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–∞–≤—ã–∫–∞—Ö"""
        if not skills_data or skills_data == '[]':
            return "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

        try:
            if isinstance(skills_data, str) and 'hardSkillName' in skills_data:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞–≤—ã–∫–æ–≤
                skills = re.findall(r"'hardSkillName': '([^']*)'", str(skills_data))
                if skills:
                    return ", ".join(skills[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–≤—ã–∫–æ–≤
            return str(skills_data)[:100] + "..." if len(str(skills_data)) > 100 else str(skills_data)
        except:
            return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏"


# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
TELEGRAM_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')

if not TELEGRAM_TOKEN:
    raise ValueError("TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã...")
search_engine = ResumeSearchEngine('–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.xlsx')
search_engine.preprocess_data()
search_engine.create_faiss_index()

# –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –∫—ç—à–∞ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
search_engine.clear_old_cache(max_age_days=30)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    welcome_text = """
ü§ñ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –±–æ—Ç –ø–æ–∏—Å–∫–∞ —Ä–µ–∑—é–º–µ!

–î–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: 
*–¥–æ–ª–∂–Ω–æ—Å—Ç—å; –≥–æ—Ä–æ–¥; –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ*

–ü—Ä–∏–º–µ—Ä—ã:
‚Ä¢ –≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—å; –ß–µ–ª—è–±–∏–Ω—Å–∫; 5
‚Ä¢ –ø–æ–º–æ—â–Ω–∏–∫ –≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—è; –ú–æ—Å–∫–≤–∞; 3
‚Ä¢ –≤–æ–¥–∏—Ç–µ–ª—å; –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥; 2

üí° *–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞:*
- –õ–æ–∫–∞—Ü–∏—è –∏–º–µ–µ—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
- –£—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
- –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ –Ω–µ—Ç–æ—á–Ω–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏
    """
    await update.message.reply_text(welcome_text)


async def handle_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        user_input = update.message.text
        parts = user_input.split(';')

        if len(parts) < 3:
            await update.message.reply_text(
                "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: [–¥–æ–ª–∂–Ω–æ—Å—Ç—å]; [–≥–æ—Ä–æ–¥]; [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ]\n\n"
                "–ü—Ä–∏–º–µ—Ä: –≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—å; –ß–µ–ª—è–±–∏–Ω—Å–∫; 5"
            )
            return

        keyword = parts[0].strip()
        location = parts[1].strip()

        try:
            k = int(parts[2].strip())
            k = min(k, 10)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        except:
            k = 5

        if not keyword or not location:
            await update.message.reply_text("‚ùå –£–∫–∞–∂–∏—Ç–µ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ")
            return

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ª–æ–∫–∞—Ü–∏–∏
        await update.message.reply_text(f"üîç –ò—â—É '{keyword}' –≤ –ª–æ–∫–∞—Ü–∏–∏ '{location}'...")

        results = search_engine.location_priority_search(keyword, location, k=k)

        if not results:
            alternative_msg = "‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n\n"
            alternative_msg += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
            alternative_msg += "‚Ä¢ –ò–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –¥–æ–ª–∂–Ω–æ—Å—Ç–∏\n"
            alternative_msg += "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π\n"
            alternative_msg += "‚Ä¢ –†–∞—Å—à–∏—Ä–∏—Ç—å —Ä–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞\n"
            alternative_msg += "‚Ä¢ –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"

            await update.message.reply_text(alternative_msg)
            return

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(results)}\n"
        response += f"üîç –ó–∞–ø—Ä–æ—Å: {keyword} –≤ {location}\n\n"

        for i, result in enumerate(results, 1):
            details = search_engine.get_resume_details(result['index'])

            if details:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ scoring –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
                score_info = f"(–ª–æ–∫–∞—Ü–∏—è: {result['location_score']:.2f}, "
                score_info += f"–ø—Ä–æ—Ñ–µ—Å—Å–∏—è: {result['profession_score']:.2f})"

                response += f"üèÜ **–†–µ–∑—é–º–µ #{i}** {score_info}\n"
                response += f"üíº **–î–æ–ª–∂–Ω–æ—Å—Ç—å:** {details['position']}\n"
                response += f"üìç **–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** {details['location']}\n"
                response += f"üë§ **–í–æ–∑—Ä–∞—Å—Ç:** {details['age']}\n"
                response += f"üìÖ **–û–ø—ã—Ç:** {details['experience']} –ª–µ—Ç\n"

                if details['salary'] and str(details['salary']) != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                    response += f"üí∞ **–ó–∞—Ä–ø–ª–∞—Ç–∞:** {details['salary']}\n"

                response += f"üéì **–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:** {details['education']}\n"
                response += f"üïí **–ì—Ä–∞—Ñ–∏–∫:** {details['schedule']}\n"
                response += f"üöó **–ü–µ—Ä–µ–µ–∑–¥:** {details['relocation']}\n"

                if i < len(results):
                    response += "‚îÄ" * 40 + "\n\n"

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
        if len(response) > 4096:
            parts = [response[i:i + 4096] for i in range(0, len(response), 4096)]
            for part in parts:
                await update.message.reply_text(part)
        else:
            await update.message.reply_text(response)

    except Exception as e:
        await update.message.reply_text(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫"""
    print(f"–û—à–∏–±–∫–∞: {context.error}")
    await update.message.reply_text("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_search))
    application.add_error_handler(error_handler)

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    application.run_polling()


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–∞
if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    print("–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã:")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ '–ü–æ–º–æ—â–Ω–∏–∫-–≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—è' –≤ –ß–µ–ª—è–±–∏–Ω—Å–∫–µ...")
    results = search_engine.location_priority_search("–ü–æ–º–æ—â–Ω–∏–∫-–≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—è", "–ß–µ–ª—è–±–∏–Ω—Å–∫", k=3)
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—é–º–µ:")
    for i, result in enumerate(results, 1):
        details = search_engine.get_resume_details(result['index'])
        print(
            f"{i}. {details['position']} - {details['location']} (–ª–æ–∫–∞—Ü–∏—è: {result['location_score']:.2f}, –ø—Ä–æ—Ñ–µ—Å—Å–∏—è: {result['profession_score']:.2f})")

    # –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫ –≤–æ–¥–∏—Ç–µ–ª–µ–π –≤ –ú–æ—Å–∫–≤–µ
    results = search_engine.location_priority_search("–≤–æ–¥–∏—Ç–µ–ª—å", "–ú–æ—Å–∫–≤–∞", k=2)
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—é–º–µ –≤–æ–¥–∏—Ç–µ–ª–µ–π –≤ –ú–æ—Å–∫–≤–µ:")
    for i, result in enumerate(results, 1):
        details = search_engine.get_resume_details(result['index'])
        print(
            f"{i}. {details['position']} - {details['location']} (–ª–æ–∫–∞—Ü–∏—è: {result['location_score']:.2f}, –ø—Ä–æ—Ñ–µ—Å—Å–∏—è: {result['profession_score']:.2f})")

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    main()